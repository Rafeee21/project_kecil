{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rafeee21/project_kecil/blob/main/sentimen_analisis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Token authenfikasi twitter\n",
        "\n",
        "twitter_auth_token = '4c9a055704a9d1515dd4b33a9b5929e4c6ca32c7'"
      ],
      "metadata": {
        "id": "xrBjnG29whv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dan install pandas\n",
        "!pip install pandas\n",
        "\n",
        "# Install Node.js (karena tweet-harvest dibangun mengunakan Node.js)\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ca-certificates curl gnupg\n",
        "!sudo mkdir -p /etc/apt/keyrings\n",
        "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "\n",
        "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install nodejs -y\n",
        "\n",
        "!node -v"
      ],
      "metadata": {
        "id": "LDwmUIZtxBSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# melakukan crawl data dari twitter\n",
        "filename = 'rohingya.csv'\n",
        "search_keyword = 'rohingya lang:id'\n",
        "limit = 1000\n",
        "!npx --yes tweet-harvest@2.2.8 -o \"{filename}\" -s \"{search_keyword}\" -l {limit} --token {twitter_auth_token}"
      ],
      "metadata": {
        "id": "gjNCuaesxSeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# menentukan file path bedasarkan crawl data yang sudah dilakukan\n",
        "file_path = f\"/content/rohingya.csv\"\n",
        "\n",
        "# membaca file csv\n",
        "rohingya = pd.read_csv(file_path, delimiter=\";\")\n",
        "\n",
        "# menampilkan dataframe\n",
        "display(rohingya)"
      ],
      "metadata": {
        "id": "gcJrqL1rnWJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cek jumlah data yang didapatkan\n",
        "num_tweets = len(rohingya)\n",
        "print(f\"Jumlah tweet dalam dataframe adalah {num_tweets}.\")"
      ],
      "metadata": {
        "id": "8t6cDZmxgIjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribusi Bahasa"
      ],
      "metadata": {
        "id": "LW8ZZyRSqFUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rohingya['lang'].value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "8HtbT5YmqEv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aktivitas waktu"
      ],
      "metadata": {
        "id": "m8qk7GAkqWsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ubah kolom 'created_at' menjadi tipe data datetime\n",
        "rohingya['created_at'] = pd.to_datetime(rohingya['created_at'])\n",
        "\n",
        "# Buat kolom baru untuk tahun, bulan, dan hari\n",
        "rohingya['year'] = rohingya['created_at'].dt.year\n",
        "rohingya['month'] = rohingya['created_at'].dt.month\n",
        "rohingya['day'] = rohingya['created_at'].dt.day\n",
        "rohingya['hour'] = rohingya['created_at'].dt.hour\n",
        "\n",
        "# Lihat distribusi tweet berdasarkan tahun, bulan, dan hari\n",
        "rohingya['year'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Tahun')\n",
        "rohingya['month'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Bulan')\n",
        "rohingya['day'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Hari')\n",
        "rohingya['hour'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Jam')\n",
        "\n",
        "# Menampilkan distribusi tweet berdasarkan tahun\n",
        "rohingya['year'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Tahun')\n",
        "plt.show()\n",
        "\n",
        "# Menampilkan distribusi tweet berdasarkan bulan\n",
        "rohingya['month'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Bulan')\n",
        "plt.show()\n",
        "\n",
        "# Menampilkan distribusi tweet berdasarkan hari\n",
        "rohingya['day'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Hari')\n",
        "plt.show()\n",
        "\n",
        "# Menampilkan distribusi tweet berdasarkan jam\n",
        "rohingya['hour'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Jam')\n",
        "plt.show()\n",
        "\n",
        " # Untuk distribusi tahun\n",
        "year_counts = rohingya['year'].value_counts()\n",
        "max_year = year_counts.idxmax()\n",
        "min_year = year_counts.idxmin()\n",
        "max_year_count = year_counts.max()\n",
        "min_year_count = year_counts.min()\n",
        "avg_year = year_counts.mean()\n",
        "\n",
        "# Untuk distribusi bulan\n",
        "month_counts = rohingya['month'].value_counts()\n",
        "max_month = month_counts.idxmax()\n",
        "min_month = month_counts.idxmin()\n",
        "max_month_count = month_counts.max()\n",
        "min_month_count = month_counts.min()\n",
        "avg_month = month_counts.mean()\n",
        "\n",
        "# Untuk distribusi hari\n",
        "day_counts = rohingya['day'].value_counts()\n",
        "max_day = day_counts.idxmax()\n",
        "min_day = day_counts.idxmin()\n",
        "max_day_count = day_counts.max()\n",
        "min_day_count = day_counts.min()\n",
        "avg_day = day_counts.mean()\n",
        "\n",
        "# Untuk distribusi jam\n",
        "hour_counts = rohingya['hour'].value_counts()\n",
        "max_hour = hour_counts.idxmax()\n",
        "min_hour = hour_counts.idxmin()\n",
        "max_hour_count = hour_counts.max()\n",
        "min_hour_count = hour_counts.min()\n",
        "avg_hour = hour_counts.mean()\n",
        "\n",
        "print(f\"Distribusi Tahun - Terbanyak: Tahun {max_year} dengan jumlah {max_year_count}, Tersedikit: Tahun {min_year} dengan jumlah {min_year_count}, Rata-rata: {avg_year}\")\n",
        "print(f\"Distribusi Bulan - Terbanyak: Bulan {max_month} dengan jumlah {max_month_count}, Tersedikit: Bulan {min_month} dengan jumlah {min_month_count}, Rata-rata: {avg_month}\")\n",
        "print(f\"Distribusi Hari - Terbanyak: Hari {max_day} dengan jumlah {max_day_count}, Tersedikit: Hari {min_day} dengan jumlah {min_day_count}, Rata-rata: {avg_day}\")\n",
        "print(f\"Distribusi Jam - Terbanyak: Jam {max_hour} dengan jumlah {max_hour_count}, Tersedikit: Jam {min_hour} dengan jumlah {min_hour_count}, Rata-rata: {avg_hour}\")\n"
      ],
      "metadata": {
        "id": "-mdyTA_2qWdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribusi retweet dan favorite"
      ],
      "metadata": {
        "id": "9QQWGaV0vBV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rohingya[['retweet_count', 'favorite_count']].plot(kind='hist', subplots=True, bins=30, rwidth=0.8)\n",
        "# Menampilkan jumlah dalam angka\n",
        "jumlah = rohingya[['retweet_count', 'favorite_count']].describe()\n",
        "print(jumlah)"
      ],
      "metadata": {
        "id": "kgk1ZCFuvA_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing"
      ],
      "metadata": {
        "id": "J7AyKq-sv44m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Membuat fungsi untuk Text Preprocessing"
      ],
      "metadata": {
        "id": "HHyPPcWu20NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "id": "goevd8o0PrC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "# Fungsi ini digunakan untuk membaca dan menggabungkan dua kamus slang dari dua file CSV \"kamus alay\" dan \"new_kamus alay\"\n",
        "# dan juga fungsi ini mengembalikan kamus slang yang digunakan untuk menggantikan kata-kata slang dalam teks.\n",
        "def bahasa_gaul():\n",
        "    slang_a = pd.read_csv('/content/new_kamusalay.csv', encoding='latin-1', names=['original', 'replacement'])\n",
        "    slang_b = pd.read_csv('/content/kamusalay.csv', usecols=['slang', 'formal'])\n",
        "    slang_b.columns = ['original', 'replacement']\n",
        "    slang_b.drop_duplicates(subset=['original'], keep='first', inplace=True, ignore_index=True)\n",
        "    slang = pd.concat([slang_a, slang_b], ignore_index=True)\n",
        "    slang.drop_duplicates(subset=['original'], keep='first', inplace=True, ignore_index=True)\n",
        "    return slang\n",
        "\n",
        "# Fungsi ini digunakan untuk membuat daftar stopwords.\n",
        "# Stopwords adalah kata-kata yang biasanya dihapus selama pra-pemrosesan teks karena mereka tidak memberikan banyak informasi untuk analisis teks.\n",
        "# Fungsi ini menggabungkan stopwords dari file CSV dan pustaka Sastrawi, dan juga memungkinkan penambahan kata-kata tambahan ke daftar stopwords.\n",
        "def bahasa_stopwords(additional_words=[]):\n",
        "    stopword_a = pd.read_csv('/content/stopwordbahasa.csv', names=['stopword'])\n",
        "    stopword = stopword_a['stopword'].tolist()\n",
        "    factory = StopWordRemoverFactory()\n",
        "    stopword_b = factory.get_stop_words()\n",
        "    stopword.extend(stopword_b)\n",
        "    if type(additional_words) != list:\n",
        "        return TypeError\n",
        "    else:\n",
        "        if len(additional_words) > 0:\n",
        "            stopword.extend(additional_words)\n",
        "    stopword = list(dict.fromkeys(stopword))\n",
        "    return stopword"
      ],
      "metadata": {
        "id": "C4S0HHXQ4pIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Fungsi ini mengubah semua karakter dalam kalimat menjadi huruf kecil.\n",
        "def get_low(sentence):\n",
        "    return sentence.lower()\n",
        "\n",
        "# Fungsi ini menghapus semua tanda baca dari kalimat.\n",
        "def del_punc(sentence):\n",
        "    return ''.join([w for w in sentence if w not in string.punctuation])\n",
        "\n",
        "# Fungsi ini menghapus \"kata-kata kotor\" atau karakter yang tidak perlu dari kalimat berdasarkan list yang dibuat berikut.\n",
        "def del_dirty_words(sentence):\n",
        "    sentence = re.sub(r'\\\\n', ' ', sentence)\n",
        "    sentence = re.sub(r'\\\\t', ' ', sentence)\n",
        "    sentence = re.sub(r'\\\"', '', sentence)\n",
        "    sentence = re.sub(r'\\[username\\]', '', sentence)\n",
        "    sentence = re.sub(r'\\[user\\]', '', sentence)\n",
        "    sentence = re.sub(r'\\[url\\]', '', sentence)\n",
        "    sentence = re.sub(r'\\\\t', ' ', sentence)\n",
        "    sentence = re.sub(r'@[A-Za-z0–9]+', '', sentence)\n",
        "    sentence = re.sub(r'\\$\\w*', '', sentence)\n",
        "    sentence = re.sub(r'(^(rt)|\\s)+(rt)+\\s', ' ', sentence)\n",
        "    sentence = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', ' ', sentence)\n",
        "    sentence = re.sub(r'#', '', sentence)\n",
        "    sentence = re.sub(r'\\\\x[*0-9a-zA-Z]+', ' ', sentence)\n",
        "    sentence = re.sub(r'user', '', sentence)\n",
        "    sentence = re.sub(r'url', ' ', sentence)\n",
        "    sentence = re.sub(r'ssl', ' ', sentence)\n",
        "    sentence = re.sub(r'[^0-9a-zA-Z]+', ' ', sentence)\n",
        "    sentence = re.sub(r'\\d+', '', sentence)\n",
        "    sentence = re.sub(r'  +', ' ', sentence)\n",
        "    return sentence\n",
        "\n",
        "# Fungsi ini menggantikan kata-kata alay dalam kalimat dengan kata-kata baku.\n",
        "# berdasarkan file csv kamus alay yang digunakan.\n",
        "def replace_alay(sentence, alay_dictionary):\n",
        "    alay_dict = dict(zip(alay_dictionary['original'], alay_dictionary['replacement']))\n",
        "    return ' '.join([alay_dict[w] if w in alay_dict else w for w in sentence.split()])\n",
        "\n",
        "# Fungsi ini melakukan stemming pada kata-kata dalam kalimat.\n",
        "# Stemming adalah proses mengubah kata ke bentuk dasarnya.\n",
        "def stemming_words(sentence):\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    return stemmer.stem(sentence)\n",
        "\n",
        "# Fungsi ini menghapus stopwords dari kalimat.\n",
        "# Stopwords adalah kata-kata yang biasanya dihapus selama pra-pemrosesan teks karena mereka tidak memberikan banyak informasi untuk analisis teks.\n",
        "def del_stopwords(sentence):\n",
        "    stopwords_list = stopwords.words('indonesian')\n",
        "    sentence = ' '.join(['' if w in stopwords_list else w for w in sentence.split()])\n",
        "    sentence = re.sub('  +', ' ', sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "\n",
        "# Fungsi ini melakukan serangkaian langkah pra-pemrosesan teks pada kalimat\n",
        "# termasuk mengubah karakter menjadi huruf kecil, menghapus kata-kata kotor,\n",
        "# menghapus tanda baca, menggantikan kata-kata alay, melakukan stemming,\n",
        "# dan menghapus stopwords\n",
        "def text_preprocessing(sentence, alay_dictionary):\n",
        "    sentence = get_low(sentence)\n",
        "    sentence = del_dirty_words(sentence)\n",
        "    sentence = del_punc(sentence)\n",
        "    sentence = replace_alay(sentence, alay_dictionary)\n",
        "    sentence = stemming_words(sentence)\n",
        "    sentence = del_stopwords(sentence)\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "koZ6P2GBv33z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Menjalankan fungsi text preprocessing yang sudah dibuat"
      ],
      "metadata": {
        "id": "jbKbew1PGYgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "T7QET7A2vCty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impor pustaka\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Mengambil kolom teks dari dataset Anda\n",
        "text_data = rohingya['full_text']\n",
        "\n",
        "# Mendapatkan kamus slang\n",
        "alay_dictionary = bahasa_gaul()\n",
        "\n",
        "# Melakukan pra-pemrosesan teks\n",
        "processed_text = text_data.apply(lambda x: text_preprocessing(x, alay_dictionary))\n",
        "\n",
        "# Menambahkan teks yang telah diproses ke DataFrame Anda\n",
        "rohingya['processed_text'] = processed_text"
      ],
      "metadata": {
        "id": "g4XAvgs-4jcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rohingya"
      ],
      "metadata": {
        "id": "Js4GItycGQ4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Mengambil attribute dari dataset rohingya\n",
        "teks_bersih = rohingya['processed_text']\n",
        "\n",
        "# Simpan kolom sebagai file CSV baru\n",
        "teks_bersih .to_csv('teks_proses.csv', index=False)\n",
        "\n",
        "#Membaca dataset\n",
        "teks_proses = pd.read_csv(\"teks_proses.csv\")"
      ],
      "metadata": {
        "id": "VEg51hIFJBIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "teks_proses = pd.read_csv(\"teks_proses.csv\")\n",
        "teks_proses"
      ],
      "metadata": {
        "id": "6a1G2p4uCoch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Cloud"
      ],
      "metadata": {
        "id": "AJW39ctEVIYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang diperlukan\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "teks =' '.join(teks_proses['processed_text'].values)\n",
        "\n",
        "# Membuat objek WordCloud\n",
        "wc = WordCloud(width=800, height=400, max_words=200, background_color='white').generate(teks)\n",
        "\n",
        "# Menampilkan WordCloud menggunakan matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Lv4yJWjVMGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "teks =' '.join(teks_proses['processed_text'].values)\n",
        "\n",
        "# Menghitung frekuensi kata\n",
        "frekuensi_kata = pd.Series(teks.split()).value_counts()\n",
        "\n",
        "# Membuat DataFrame dari frekuensi kata\n",
        "df = pd.DataFrame({'Kata': frekuensi_kata.index, 'Frekuensi': frekuensi_kata.values})\n",
        "\n",
        "# Membuat plot frekuensi kata\n",
        "frekuensi_kata[:5].plot(kind='barh', figsize=(10, 5))\n",
        "plt.title('5 Kata Paling Sering Muncul')\n",
        "plt.xlabel('Kata')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.show()\n",
        "\n",
        "# Menampilkan 5 kata teratas\n",
        "print(df.head(5))\n"
      ],
      "metadata": {
        "id": "nhHfOZQpubA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pelabelan dataset"
      ],
      "metadata": {
        "id": "sASO6XNMAwyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Membaca file kata positif dan negatif\n",
        "with open('kata_positif.txt', 'r') as f:\n",
        "    kata_positif = f.read().splitlines()\n",
        "with open('kata_negatif.txt', 'r') as f:\n",
        "    kata_negatif = f.read().splitlines()\n",
        "\n",
        "# Fungsi untuk melakukan pelabelan\n",
        "def label_sentimen(teks):\n",
        "    for kata in teks.split():\n",
        "        if kata in kata_positif:\n",
        "            return 'positif'\n",
        "        elif kata in kata_negatif:\n",
        "            return 'negatif'\n",
        "    return 'netral'\n",
        "\n",
        "# Melakukan pelabelan\n",
        "teks_proses['sentimen'] = teks_proses['processed_text'].apply(label_sentimen)\n",
        "\n",
        "# Menampilkan DataFrame\n",
        "teks_proses\n"
      ],
      "metadata": {
        "id": "vEIyBDAANNbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Membagi data menjadi data training dan data testing dengan rasio 8:2\n",
        "X_train, X_test, y_train, y_test = train_test_split(teks_proses['processed_text'], teks_proses['sentimen'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Mengubah teks menjadi vektor menggunakan TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Membuat dan melatih model SVM\n",
        "model = SVC()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Memprediksi data testing\n",
        "predictions = model.predict(X_test_vec)\n",
        "\n",
        "# Menampilkan laporan klasifikasi\n",
        "print(classification_report(y_test, predictions))\n"
      ],
      "metadata": {
        "id": "1Z_xWxGgU-lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Menghitung jumlah sentimen\n",
        "jumlah_sentimen = teks_proses['sentimen'].value_counts()\n",
        "\n",
        "# Menampilkan jumlah sentimen dalam statistik\n",
        "print(jumlah_sentimen)\n",
        "\n",
        "# Membuat grafik batang dari jumlah sentimen\n",
        "plt.figure(figsize=(8, 6))\n",
        "jumlah_sentimen.plot(kind='bar')\n",
        "plt.title('Distribusi Sentimen')\n",
        "plt.xlabel('Sentimen')\n",
        "plt.ylabel('Jumlah')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O9s2FFR3J39J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}