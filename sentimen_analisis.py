# -*- coding: utf-8 -*-
"""sentimen_analisis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sx8AOziBZq_j7z5vGJnmWNy01qMg5s0X
"""

#Token authenfikasi twitter

twitter_auth_token = '4c9a055704a9d1515dd4b33a9b5929e4c6ca32c7'

# Import dan install pandas
!pip install pandas

# Install Node.js (karena tweet-harvest dibangun mengunakan Node.js)
!sudo apt-get update
!sudo apt-get install -y ca-certificates curl gnupg
!sudo mkdir -p /etc/apt/keyrings
!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg

!NODE_MAJOR=20 && echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main" | sudo tee /etc/apt/sources.list.d/nodesource.list

!sudo apt-get update
!sudo apt-get install nodejs -y

!node -v

# melakukan crawl data dari twitter
filename = 'rohingya.csv'
search_keyword = 'rohingya lang:id'
limit = 1000
!npx --yes tweet-harvest@2.2.8 -o "{filename}" -s "{search_keyword}" -l {limit} --token {twitter_auth_token}

import pandas as pd

# menentukan file path bedasarkan crawl data yang sudah dilakukan
file_path = f"/content/rohingya.csv"

# membaca file csv
rohingya = pd.read_csv(file_path, delimiter=";")

# menampilkan dataframe
display(rohingya)

# Cek jumlah data yang didapatkan
num_tweets = len(rohingya)
print(f"Jumlah tweet dalam dataframe adalah {num_tweets}.")

"""Distribusi Bahasa"""

rohingya['lang'].value_counts().plot(kind='bar')

"""Aktivitas waktu"""

import matplotlib.pyplot as plt

# Ubah kolom 'created_at' menjadi tipe data datetime
rohingya['created_at'] = pd.to_datetime(rohingya['created_at'])

# Buat kolom baru untuk tahun, bulan, dan hari
rohingya['year'] = rohingya['created_at'].dt.year
rohingya['month'] = rohingya['created_at'].dt.month
rohingya['day'] = rohingya['created_at'].dt.day
rohingya['hour'] = rohingya['created_at'].dt.hour

# Lihat distribusi tweet berdasarkan tahun, bulan, dan hari
rohingya['year'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Tahun')
rohingya['month'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Bulan')
rohingya['day'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Hari')
rohingya['hour'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Jam')

# Menampilkan distribusi tweet berdasarkan tahun
rohingya['year'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Tahun')
plt.show()

# Menampilkan distribusi tweet berdasarkan bulan
rohingya['month'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Bulan')
plt.show()

# Menampilkan distribusi tweet berdasarkan hari
rohingya['day'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Hari')
plt.show()

# Menampilkan distribusi tweet berdasarkan jam
rohingya['hour'].value_counts().sort_index().plot(kind='barh', title='Distribusi Tweet Berdasarkan Jam')
plt.show()

 # Untuk distribusi tahun
year_counts = rohingya['year'].value_counts()
max_year = year_counts.idxmax()
min_year = year_counts.idxmin()
max_year_count = year_counts.max()
min_year_count = year_counts.min()
avg_year = year_counts.mean()

# Untuk distribusi bulan
month_counts = rohingya['month'].value_counts()
max_month = month_counts.idxmax()
min_month = month_counts.idxmin()
max_month_count = month_counts.max()
min_month_count = month_counts.min()
avg_month = month_counts.mean()

# Untuk distribusi hari
day_counts = rohingya['day'].value_counts()
max_day = day_counts.idxmax()
min_day = day_counts.idxmin()
max_day_count = day_counts.max()
min_day_count = day_counts.min()
avg_day = day_counts.mean()

# Untuk distribusi jam
hour_counts = rohingya['hour'].value_counts()
max_hour = hour_counts.idxmax()
min_hour = hour_counts.idxmin()
max_hour_count = hour_counts.max()
min_hour_count = hour_counts.min()
avg_hour = hour_counts.mean()

print(f"Distribusi Tahun - Terbanyak: Tahun {max_year} dengan jumlah {max_year_count}, Tersedikit: Tahun {min_year} dengan jumlah {min_year_count}, Rata-rata: {avg_year}")
print(f"Distribusi Bulan - Terbanyak: Bulan {max_month} dengan jumlah {max_month_count}, Tersedikit: Bulan {min_month} dengan jumlah {min_month_count}, Rata-rata: {avg_month}")
print(f"Distribusi Hari - Terbanyak: Hari {max_day} dengan jumlah {max_day_count}, Tersedikit: Hari {min_day} dengan jumlah {min_day_count}, Rata-rata: {avg_day}")
print(f"Distribusi Jam - Terbanyak: Jam {max_hour} dengan jumlah {max_hour_count}, Tersedikit: Jam {min_hour} dengan jumlah {min_hour_count}, Rata-rata: {avg_hour}")

"""Distribusi retweet dan favorite"""

rohingya[['retweet_count', 'favorite_count']].plot(kind='hist', subplots=True, bins=30, rwidth=0.8)
# Menampilkan jumlah dalam angka
jumlah = rohingya[['retweet_count', 'favorite_count']].describe()
print(jumlah)

"""Text Preprocessing

Membuat fungsi untuk Text Preprocessing
"""

!pip install Sastrawi

import pandas as pd
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

# Fungsi ini digunakan untuk membaca dan menggabungkan dua kamus slang dari dua file CSV "kamus alay" dan "new_kamus alay"
# dan juga fungsi ini mengembalikan kamus slang yang digunakan untuk menggantikan kata-kata slang dalam teks.
def bahasa_gaul():
    slang_a = pd.read_csv('/content/new_kamusalay.csv', encoding='latin-1', names=['original', 'replacement'])
    slang_b = pd.read_csv('/content/kamusalay.csv', usecols=['slang', 'formal'])
    slang_b.columns = ['original', 'replacement']
    slang_b.drop_duplicates(subset=['original'], keep='first', inplace=True, ignore_index=True)
    slang = pd.concat([slang_a, slang_b], ignore_index=True)
    slang.drop_duplicates(subset=['original'], keep='first', inplace=True, ignore_index=True)
    return slang

# Fungsi ini digunakan untuk membuat daftar stopwords.
# Stopwords adalah kata-kata yang biasanya dihapus selama pra-pemrosesan teks karena mereka tidak memberikan banyak informasi untuk analisis teks.
# Fungsi ini menggabungkan stopwords dari file CSV dan pustaka Sastrawi, dan juga memungkinkan penambahan kata-kata tambahan ke daftar stopwords.
def bahasa_stopwords(additional_words=[]):
    stopword_a = pd.read_csv('/content/stopwordbahasa.csv', names=['stopword'])
    stopword = stopword_a['stopword'].tolist()
    factory = StopWordRemoverFactory()
    stopword_b = factory.get_stop_words()
    stopword.extend(stopword_b)
    if type(additional_words) != list:
        return TypeError
    else:
        if len(additional_words) > 0:
            stopword.extend(additional_words)
    stopword = list(dict.fromkeys(stopword))
    return stopword

import pandas as pd
import numpy as np
import string
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
import re
from nltk.corpus import stopwords

# Fungsi ini mengubah semua karakter dalam kalimat menjadi huruf kecil.
def get_low(sentence):
    return sentence.lower()

# Fungsi ini menghapus semua tanda baca dari kalimat.
def del_punc(sentence):
    return ''.join([w for w in sentence if w not in string.punctuation])

# Fungsi ini menghapus "kata-kata kotor" atau karakter yang tidak perlu dari kalimat berdasarkan list yang dibuat berikut.
def del_dirty_words(sentence):
    sentence = re.sub(r'\\n', ' ', sentence)
    sentence = re.sub(r'\\t', ' ', sentence)
    sentence = re.sub(r'\"', '', sentence)
    sentence = re.sub(r'\[username\]', '', sentence)
    sentence = re.sub(r'\[user\]', '', sentence)
    sentence = re.sub(r'\[url\]', '', sentence)
    sentence = re.sub(r'\\t', ' ', sentence)
    sentence = re.sub(r'@[A-Za-z0â€“9]+', '', sentence)
    sentence = re.sub(r'\$\w*', '', sentence)
    sentence = re.sub(r'(^(rt)|\s)+(rt)+\s', ' ', sentence)
    sentence = re.sub(r'((www\.[^\s]+)|(https?://[^\s]+)|(http?://[^\s]+))', ' ', sentence)
    sentence = re.sub(r'#', '', sentence)
    sentence = re.sub(r'\\x[*0-9a-zA-Z]+', ' ', sentence)
    sentence = re.sub(r'user', '', sentence)
    sentence = re.sub(r'url', ' ', sentence)
    sentence = re.sub(r'ssl', ' ', sentence)
    sentence = re.sub(r'[^0-9a-zA-Z]+', ' ', sentence)
    sentence = re.sub(r'\d+', '', sentence)
    sentence = re.sub(r'  +', ' ', sentence)
    return sentence

# Fungsi ini menggantikan kata-kata alay dalam kalimat dengan kata-kata baku.
# berdasarkan file csv kamus alay yang digunakan.
def replace_alay(sentence, alay_dictionary):
    alay_dict = dict(zip(alay_dictionary['original'], alay_dictionary['replacement']))
    return ' '.join([alay_dict[w] if w in alay_dict else w for w in sentence.split()])

# Fungsi ini melakukan stemming pada kata-kata dalam kalimat.
# Stemming adalah proses mengubah kata ke bentuk dasarnya.
def stemming_words(sentence):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    return stemmer.stem(sentence)

# Fungsi ini menghapus stopwords dari kalimat.
# Stopwords adalah kata-kata yang biasanya dihapus selama pra-pemrosesan teks karena mereka tidak memberikan banyak informasi untuk analisis teks.
def del_stopwords(sentence):
    stopwords_list = stopwords.words('indonesian')
    sentence = ' '.join(['' if w in stopwords_list else w for w in sentence.split()])
    sentence = re.sub('  +', ' ', sentence)
    sentence = sentence.strip()
    return sentence

# Fungsi ini melakukan serangkaian langkah pra-pemrosesan teks pada kalimat
# termasuk mengubah karakter menjadi huruf kecil, menghapus kata-kata kotor,
# menghapus tanda baca, menggantikan kata-kata alay, melakukan stemming,
# dan menghapus stopwords
def text_preprocessing(sentence, alay_dictionary):
    sentence = get_low(sentence)
    sentence = del_dirty_words(sentence)
    sentence = del_punc(sentence)
    sentence = replace_alay(sentence, alay_dictionary)
    sentence = stemming_words(sentence)
    sentence = del_stopwords(sentence)
    return sentence

"""Menjalankan fungsi text preprocessing yang sudah dibuat"""

!pip install nltk
import nltk
nltk.download('stopwords')

# Impor pustaka
import pandas as pd
import numpy as np
import string
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
import re
from nltk.corpus import stopwords

# Mengambil kolom teks dari dataset Anda
text_data = rohingya['full_text']

# Mendapatkan kamus slang
alay_dictionary = bahasa_gaul()

# Melakukan pra-pemrosesan teks
processed_text = text_data.apply(lambda x: text_preprocessing(x, alay_dictionary))

# Menambahkan teks yang telah diproses ke DataFrame Anda
rohingya['processed_text'] = processed_text

rohingya

# Import library pandas
import pandas as pd

# Mengambil attribute dari dataset rohingya
teks_bersih = rohingya['processed_text']

# Simpan kolom sebagai file CSV baru
teks_bersih .to_csv('teks_proses.csv', index=False)

#Membaca dataset
teks_proses = pd.read_csv("teks_proses.csv")

teks_proses = pd.read_csv("teks_proses.csv")
teks_proses

"""Word Cloud"""

# Import library yang diperlukan
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from PIL import Image
import numpy as np

teks =' '.join(teks_proses['processed_text'].values)

# Membuat objek WordCloud
wc = WordCloud(width=800, height=400, max_words=200, background_color='white').generate(teks)

# Menampilkan WordCloud menggunakan matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()

import pandas as pd

teks =' '.join(teks_proses['processed_text'].values)

# Menghitung frekuensi kata
frekuensi_kata = pd.Series(teks.split()).value_counts()

# Membuat DataFrame dari frekuensi kata
df = pd.DataFrame({'Kata': frekuensi_kata.index, 'Frekuensi': frekuensi_kata.values})

# Membuat plot frekuensi kata
frekuensi_kata[:5].plot(kind='barh', figsize=(10, 5))
plt.title('5 Kata Paling Sering Muncul')
plt.xlabel('Kata')
plt.ylabel('Frekuensi')
plt.show()

# Menampilkan 5 kata teratas
print(df.head(5))

"""Pelabelan dataset"""

import pandas as pd

# Membaca file kata positif dan negatif
with open('kata_positif.txt', 'r') as f:
    kata_positif = f.read().splitlines()
with open('kata_negatif.txt', 'r') as f:
    kata_negatif = f.read().splitlines()

# Fungsi untuk melakukan pelabelan
def label_sentimen(teks):
    for kata in teks.split():
        if kata in kata_positif:
            return 'positif'
        elif kata in kata_negatif:
            return 'negatif'
    return 'netral'

# Melakukan pelabelan
teks_proses['sentimen'] = teks_proses['processed_text'].apply(label_sentimen)

# Menampilkan DataFrame
teks_proses

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import classification_report

# Membagi data menjadi data training dan data testing dengan rasio 8:2
X_train, X_test, y_train, y_test = train_test_split(teks_proses['processed_text'], teks_proses['sentimen'], test_size=0.2, random_state=42)

# Mengubah teks menjadi vektor menggunakan TF-IDF
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# Membuat dan melatih model SVM
model = SVC()
model.fit(X_train_vec, y_train)

# Memprediksi data testing
predictions = model.predict(X_test_vec)

# Menampilkan laporan klasifikasi
print(classification_report(y_test, predictions))

import pandas as pd
import matplotlib.pyplot as plt

# Menghitung jumlah sentimen
jumlah_sentimen = teks_proses['sentimen'].value_counts()

# Menampilkan jumlah sentimen dalam statistik
print(jumlah_sentimen)

# Membuat grafik batang dari jumlah sentimen
plt.figure(figsize=(8, 6))
jumlah_sentimen.plot(kind='bar')
plt.title('Distribusi Sentimen')
plt.xlabel('Sentimen')
plt.ylabel('Jumlah')
plt.show()